{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Disaster Tweets Classification\n",
    "## Natural Language Processing with Recurrent Neural Networks\n",
    "\n",
    "**Author**: Matthew Campbell  \n",
    "**Course**: Week 14 Module 4 - RNNs and NLP  \n",
    "**Date**: October 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Project Description](#1-project-description)\n",
    "2. [Data Description](#2-data-description)\n",
    "3. [Exploratory Data Analysis](#3-exploratory-data-analysis)\n",
    "4. [Data Preprocessing](#4-data-preprocessing)\n",
    "5. [Model Architecture](#5-model-architecture)\n",
    "6. [Results and Analysis](#6-results-and-analysis)\n",
    "7. [Conclusion](#7-conclusion)\n",
    "8. [GitHub Repository](#8-github-repository)\n",
    "9. [Kaggle Submission](#9-kaggle-submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Google Colab Setup (uncomment if running in Colab)\n# from google.colab import drive\n# drive.mount('/content/drive')\n# %cd /content/drive/MyDrive/Boulder_University/Week14_Module4\n\n# Install packages if needed in Colab\n# !pip install -q wordcloud",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Setup: Google Colab vs Local\n\nThis notebook can run in both Google Colab and locally. If using Colab, uncomment the cell below to mount Google Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Check if running in Google Colab\ntry:\n    import google.colab\n    IN_COLAB = True\n    print(\"Running in Google Colab\")\nexcept:\n    IN_COLAB = False\n    print(\"Running locally\")\n\n# Set data directory\n# If you mounted Google Drive in Colab, the data is in the 'data' subdirectory\n# If running locally, data is also in 'data' subdirectory\ndata_dir = Path('data')\n\nprint(f\"Data directory: {data_dir}\")\n\n# Load data\ntrain_df = pd.read_csv(data_dir / 'train.csv')\ntest_df = pd.read_csv(data_dir / 'test.csv')\nsample_submission = pd.read_csv(data_dir / 'sample_submission.csv')\n\nprint(\"‚úì Data loaded successfully!\")\nprint(f\"Training set: {len(train_df):,} samples\")\nprint(f\"Test set: {len(test_df):,} samples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Project Description\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "This project addresses the Kaggle [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) competition. The goal is to build a machine learning model that can accurately predict whether a given tweet is about a real disaster or not.\n",
    "\n",
    "### Context\n",
    "Social media platforms like Twitter have become important communication channels during emergency events. However, not all tweets containing disaster-related keywords actually refer to real disasters. For example:\n",
    "- \"Our city is on fire with excitement!\" ‚ùå Not a disaster\n",
    "- \"There's a bushfire approaching the town\" ‚úÖ Real disaster\n",
    "\n",
    "Being able to automatically identify genuine disaster tweets can help:\n",
    "- Emergency services respond faster\n",
    "- News organisations verify events\n",
    "- Aid organisations deploy resources effectively\n",
    "\n",
    "### Problem Type\n",
    "**Binary text classification** using Natural Language Processing and Recurrent Neural Networks.\n",
    "\n",
    "### Evaluation Metric\n",
    "Models will be evaluated using **F1 Score** (harmonic mean of precision and recall). This metric is appropriate because:\n",
    "- It balances false positives and false negatives\n",
    "- Both types of errors matter in disaster detection\n",
    "- It handles class imbalance better than accuracy\n",
    "\n",
    "### Dataset Overview\n",
    "- **Training samples**: 7,613 labelled tweets\n",
    "- **Test samples**: 3,263 unlabelled tweets (for Kaggle submission)\n",
    "- **Features**: Tweet text, keywords, location (optional)\n",
    "- **Labels**: 1 = real disaster, 0 = not a disaster\n",
    "\n",
    "### Technical Approach\n",
    "We'll explore Recurrent Neural Networks (RNNs), specifically:\n",
    "1. **LSTM** (Long Short-Term Memory) - handles long-term dependencies\n",
    "2. **GRU** (Gated Recurrent Unit) - simpler, faster alternative to LSTM\n",
    "3. **Bidirectional RNNs** - process text in both directions for better context\n",
    "\n",
    "These architectures are well-suited for sequential text data because they can:\n",
    "- Capture word order and context\n",
    "- Handle variable-length inputs\n",
    "- Learn long-range dependencies between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data Description\n",
    "\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if running in Google Colab\ntry:\n    import google.colab\n    IN_COLAB = True\n    print(\"Running in Google Colab\")\nexcept:\n    IN_COLAB = False\n    print(\"Running locally\")\n\n# Set data directory based on environment\nif IN_COLAB:\n    # In Colab, data is typically in /content or mounted drive\n    # Assuming data is uploaded to Colab's content folder\n    data_dir = Path('/content')\nelse:\n    # Local environment\n    data_dir = Path('data')\n\nprint(f\"Data directory: {data_dir}\")\n\n# Load data\ntrain_df = pd.read_csv(data_dir / 'train.csv')\ntest_df = pd.read_csv(data_dir / 'test.csv')\nsample_submission = pd.read_csv(data_dir / 'sample_submission.csv')\n\nprint(\"‚úì Data loaded successfully!\")\nprint(f\"Training set: {len(train_df):,} samples\")\nprint(f\"Test set: {len(test_df):,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Training set size: {len(train_df):,} samples\")\n",
    "print(f\"üìä Test set size: {len(test_df):,} samples\")\n",
    "print(f\"üìä Total samples: {len(train_df) + len(test_df):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "print(f\"\\nData types:\\n{train_df.dtypes}\")\n",
    "print(f\"\\nShape: {train_df.shape}\")\n",
    "print(f\"Memory usage: {train_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few rows\n",
    "print(\"First 10 training samples:\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions and Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse text length\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìù Character length:\")\n",
    "print(f\"   Mean: {train_df['text_length'].mean():.1f} characters\")\n",
    "print(f\"   Median: {train_df['text_length'].median():.1f} characters\")\n",
    "print(f\"   Min: {train_df['text_length'].min()} characters\")\n",
    "print(f\"   Max: {train_df['text_length'].max()} characters\")\n",
    "print(f\"   Std: {train_df['text_length'].std():.1f}\")\n",
    "\n",
    "print(f\"\\nüìù Word count:\")\n",
    "print(f\"   Mean: {train_df['word_count'].mean():.1f} words\")\n",
    "print(f\"   Median: {train_df['word_count'].median():.1f} words\")\n",
    "print(f\"   Min: {train_df['word_count'].min()} words\")\n",
    "print(f\"   Max: {train_df['word_count'].max()} words\")\n",
    "print(f\"   Std: {train_df['word_count'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "missing_counts = train_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(train_df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(f\"\\n{missing_df}\")\n",
    "\n",
    "print(\"\\nüí° Note: 'keyword' and 'location' fields have missing values.\")\n",
    "print(\"   We'll focus on 'text' field which has no missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(f\"\\n0 (Not disaster): {class_counts[0]:,} samples ({class_counts[0]/len(train_df)*100:.1f}%)\")\n",
    "print(f\"1 (Disaster):     {class_counts[1]:,} samples ({class_counts[1]/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"‚ö†Ô∏è  Moderate class imbalance detected\")\n",
    "    print(\"    ‚Üí Will use stratified split to maintain class balance\")\n",
    "    print(\"    ‚Üí May consider class weights during training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}